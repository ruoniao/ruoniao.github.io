---
clayout    : post
title     : "OpenVLA笔记"
date      : 2026-02-23
lastupdate: 2026-02-23
categories: openvla
---
----

* TOC
{:toc}

----
| 最近在看vla相关的模型，了解到openvla,有必要写篇博客记录一下，为什么说是站在巨人的肩膀上的一个项目，从数据来源，处理，到模型的构建，融合，再到训练和微调的流程，最后到模型的验证，性能测试都能看到业界优秀作者的智慧集合。同时openvla做了大量的数据层面和模型层面的消融实验，从中能获取到一些vla的一些启示。


# 数据源
- `trajectory`（轨迹）：机器人从开始到结束的一段完整操作记录，通常包含图像、语言指令、动作序列。
- `RLDS`：一种常见的机器人数据组织格式，可以把不同来源的数据用统一结构表示。
- `demo`（示教）：人类或脚本演示给机器人的样例，一条 demo 往往对应一个任务执行过程。

| 数据源 | 用途 | 数据量（约） | 说明 |
|---|---|---:|---|
| Open X-Embodiment (OXE) 混合数据（RLDS） | 通用预训练 | `970K trajectories` | 可以把它理解为“机器人领域的大型通识语料库”：覆盖多机器人、多任务、多场景。模型先在这里学会基础视觉-语言-动作对齐能力。 |
| BridgeData V2 | 任务/场景微调 | `124 GB` | 更贴近真实操作场景的数据集，常用于把通用能力收敛到具体机器人平台和任务分布上。 |
| modified LIBERO RLDS | 仿真基准评测与微调 | `~10 GB` | 偏标准化 benchmark 数据，适合做可复现实验和横向对比。 |
| 目标域机器人示教数据（自建） | 最后阶段域适配 | `~100 demos`（经验值） | 当你把模型迁移到自己的机械臂/相机/工位时，通常还需要少量本地示教，让模型适应“你的环境细节”。 |

如果只看量级，可以把它分成三层：
1. 通识层：几十万到百万级轨迹（学“常识”）。
2. 领域层：10GB~100GB 级任务数据（学“本领域习惯”）。
3. 本地层：几十到几百条示教（学“你的设备和现场”）。

# 数据处理
OpenVLA 用“每数据集一个标准化函数 + 全局统一字段协议 + 加权混采 + 统一token化训练目标”来吃掉多源异构数据。新增数据源时，主要是补 configs + transform + mixture，主训练代码基本不用改。
OpenVLA 的统一思路是：先把“异构机器人数据”映射成同一中间格式（RLDS标准字段），再走同一条训练管线。
- 核心入口函数 prismatic\vla\datasets\rlds\dataset.py make_interleaved_dataset加载数据


## 1.数据源注册
- 每个数据集先在配置表里定义观测/动作字段：configs.py (line 70)
```python
OXE_DATASET_CONFIGS = {
    "bridge_oxe": {  # Version of Bridge V2 in Open X-Embodiment mixture
        "image_obs_keys": {"primary": "image", "secondary": "image_1", "wrist": None},
        "depth_obs_keys": {"primary": None, "secondary": None, "wrist": None},
        "state_obs_keys": ["EEF_state", None, "gripper_state"],
        "state_encoding": StateEncoding.POS_EULER,
        "action_encoding": ActionEncoding.EEF_POS,
    }
}
```
- 每个数据集对应一个标准化函数（把原始字段改成统一键名/结构）：transforms.py (line 865)
```python
# === Registry ===
OXE_STANDARDIZATION_TRANSFORMS = {
    "bridge_oxe": bridge_oxe_dataset_transform
}
def bridge_oxe_dataset_transform(trajectory: Dict[str, Any]) -> Dict[str, Any]:
    """
    Applies to version of Bridge V2 in Open X-Embodiment mixture.

    Note =>> In original Bridge V2 dataset, the first timestep has an all-zero action, so we remove it!
    """
    for key in trajectory.keys():
        if key == "traj_metadata":
            continue
        elif key in ["observation", "action"]:
            for key2 in trajectory[key]:
                trajectory[key][key2] = trajectory[key][key2][1:]
        else:
            trajectory[key] = trajectory[key][1:]

    trajectory["action"] = tf.concat(
        (
            trajectory["action"]["world_vector"],
            trajectory["action"]["rotation_delta"],
            tf.cast(trajectory["action"]["open_gripper"][:, None], tf.float32),
        ),
        axis=-1,
    )
    trajectory["language_instruction"] = trajectory["observation"]["natural_language_instruction"]
    trajectory = relabel_bridge_actions(trajectory)
    trajectory["observation"]["EEF_state"] = trajectory["observation"]["state"][:, :6]
    trajectory["observation"]["gripper_state"] = trajectory["observation"]["state"][:, -1:]
    return trajectory
```
- 组装时把两者绑定到 dataset kwargs：materialize.py (line 37), materialize.py (line 86)
```python
def make_oxe_dataset_kwargs(
    dataset_name: str,
    data_root_dir: Path,
    load_camera_views: Tuple[str] = ("primary",),
    load_depth: bool = False,
    load_proprio: bool = True,
    load_language: bool = True,
    action_proprio_normalization_type: NormalizationType = NormalizationType.NORMAL,
) -> Dict[str, Any]:
    """Generates config (kwargs) for given dataset from Open-X Embodiment."""
    dataset_kwargs = deepcopy(OXE_DATASET_CONFIGS[dataset_name])
    if dataset_kwargs["action_encoding"] not in [ActionEncoding.EEF_POS, ActionEncoding.EEF_R6]:
        raise ValueError(f"Cannot load `{dataset_name}`; only EEF_POS & EEF_R6 actions supported!")

    # [Contract] For EEF_POS & EEF_R6 actions, only the last action dimension (gripper) is absolute!
    # Normalize all action dimensions *except* the gripper
    if dataset_kwargs["action_encoding"] is ActionEncoding.EEF_POS:
        dataset_kwargs["absolute_action_mask"] = [False] * 6 + [True]
        dataset_kwargs["action_normalization_mask"] = [True] * 6 + [False]
    elif dataset_kwargs["action_encoding"] is ActionEncoding.EEF_R6:
        dataset_kwargs["absolute_action_mask"] = [False] * 9 + [True]
        dataset_kwargs["action_normalization_mask"] = [True] * 9 + [False]
    dataset_kwargs["action_proprio_normalization_type"] = action_proprio_normalization_type

    ......

    # Specify Standardization Transform
    # 绑定函数为自定义的函数
    dataset_kwargs["standardize_fn"] = OXE_STANDARDIZATION_TRANSFORMS[dataset_name]
    ...
```



## 2.统一动作语义与归一化
###  机器人动作表示（Action Encoding）方式
```python
# Defines Action Encoding Schemes
class ActionEncoding(IntEnum):
    # fmt: off
    EEF_POS = 1             # EEF Delta XYZ (3) + Roll-Pitch-Yaw (3) + Gripper Open/Close (1)
    JOINT_POS = 2           # Joint Delta Position (7) + Gripper Open/Close (1)
    JOINT_POS_BIMANUAL = 3  # Joint Delta Position (2 x [ Joint Delta Position (6) + Gripper Open/Close (1) ])
    EEF_R6 = 4              # EEF Delta XYZ (3) + R6 (6) + Gripper Open/Close (1)
    # fmt: on
```
| 编码方式                   | 表示形式                                            | 含义                         | 适用场景                             | 优点                                      | 缺点                                             |
| ---------------------- | ----------------------------------------------- | -------------------------- | -------------------------------- | --------------------------------------- | ---------------------------------------------- |
| **EEF_POS**            | Δx, Δy, Δz + Δroll, Δpitch, Δyaw + gripper (7维) | 末端执行器在任务空间的相对位姿变化（欧拉角表示旋转） | 多机器人泛化、VLA模型、任务空间控制              | ✔ 结构无关<br>✔ 易跨机器人泛化<br>✔ 语义清晰           | ✘ 欧拉角存在万向节锁（gimbal lock）<br>✘ 旋转不连续<br>✘ 需IK求解 |
| **JOINT_POS**          | Δθ₁…Δθ₇ + gripper (8维)                          | 每个关节角度的相对变化                | 单机器人训练、精细控制                      | ✔ 控制精确<br>✔ 不需要IK<br>✔ 稳定性高             | ✘ 不同机器人关节数不同<br>✘ 泛化能力弱<br>✘ 结构强耦合             |
| **JOINT_POS_BIMANUAL** | 2 × (Δθ₁…Δθ₆ + gripper) (14维)                   | 双臂机器人每侧关节角度变化              | 双臂协作任务、Humanoid上肢                | ✔ 支持双臂协调<br>✔ 直接控制                      | ✘ 维度高<br>✘ 泛化更难<br>✘ 需严格关节顺序标准化                |
| **EEF_R6**             | Δx, Δy, Δz + R6(6维) + gripper (10维)             | 末端执行器任务空间位姿变化，旋转用6D连续表示    | 大规模VLA、跨embodiment训练、需要高旋转稳定性的任务 | ✔ 无万向节锁<br>✔ 连续光滑<br>✔ 神经网络友好<br>✔ 结构无关 | ✘ 维度更高<br>✘ 需转换为旋转矩阵<br>✘ 推理阶段仍需IK             |

- 泛化能力 EEF_R6 ≥ EEF_POS >> JOINT_POS > JOINT_POS_BIMANUAL
- R6 是一种旋转表示方法，不是独立的控制空间。EEF_R6 才是完整的动作编码方式。

### 统一动作语义
OpenVLA 在数据入口先做“动作语义对齐”，核心目标是让不同机器人平台的动作都映射到统一控制语义上。  
在 `prismatic/vla/datasets/rlds/oxe/materialize.py:47` 可以看到，它只接受两种末端执行器动作编码：
- `EEF_POS`：末端位姿增量（位置+姿态）+ 夹爪控制。
- `EEF_R6`：6D 旋转表示相关的末端动作 + 夹爪控制。
```python
def make_oxe_dataset_kwargs(
    dataset_name: str,
    data_root_dir: Path,
    load_camera_views: Tuple[str] = ("primary",),
    load_depth: bool = False,
    load_proprio: bool = True,
    load_language: bool = True,
    action_proprio_normalization_type: NormalizationType = NormalizationType.NORMAL,
) -> Dict[str, Any]:
    """Generates config (kwargs) for given dataset from Open-X Embodiment."""
    dataset_kwargs = deepcopy(OXE_DATASET_CONFIGS[dataset_name])
    if dataset_kwargs["action_encoding"] not in [ActionEncoding.EEF_POS, ActionEncoding.EEF_R6]:
        # 非 ActionEncoding.EEF_POS, ActionEncoding.EEF_R6 不支持
        raise ValueError(f"Cannot load `{dataset_name}`; only EEF_POS & EEF_R6 actions supported!")
    
```
这样做的好处是：模型看到的动作 token 空间始终一致，不会因为某个数据集动作定义特殊而破坏训练稳定性。

同一段代码还定义了两个关键 mask：
- `absolute_action_mask`：哪些动作维度是“绝对量”。这里最后一维（通常是 `gripper`）被当作绝对量。
- `action_normalization_mask`：哪些维度参与归一化。通常除了 `gripper` 外都归一化。

直观理解：
- 机械臂位姿增量是连续控制量，跨数据集尺度差异大，必须归一化。
- 夹爪开合往往是离散/二值语义（开/关），不适合按连续值做同样归一化。

### 归一化
归一化在通用 RLDS 管线里统一执行，避免“每个数据集各写一套”导致分布不一致。

入口在 `prismatic/vla/datasets/rlds/dataset.py:59` 的 `make_dataset_from_rlds`：
```python
def make_dataset_from_rlds(
    name: str,
    data_dir: str,
    *,
    train: bool,
    standardize_fn: Optional[Callable[[dict], dict]] = None,
    shuffle: bool = True,
    image_obs_keys: Dict[str, Optional[str]] = {},
    depth_obs_keys: Dict[str, Optional[str]] = {},
    state_obs_keys: List[Optional[str]] = (),
    language_key: Optional[str] = None,
    action_proprio_normalization_type: NormalizationType = NormalizationType.NORMAL,
    dataset_statistics: Optional[Union[dict, str]] = None,
    absolute_action_mask: Optional[List[bool]] = None,
    action_normalization_mask: Optional[List[bool]] = None,
    num_parallel_reads: int = tf.data.AUTOTUNE,
    num_parallel_calls: int = tf.data.AUTOTUNE,
) -> Tuple[dl.DLataset, dict]:
    ...
    # -----------------------------------------------------------
    # 从 RLDS 格式的数据构建 DLataset
    #
    # builder: RLDS 数据构建器
    # split:   数据划分（如 "train" / "validation"）
    # shuffle: 是否打乱数据
    # num_parallel_reads: 并行读取线程数
    #
    # 这里得到的是“按轨迹组织”的数据集
    # -----------------------------------------------------------
    dataset = dl.DLataset.from_rlds(
        builder,
        split=split,
        shuffle=shuffle,
        num_parallel_reads=num_parallel_reads
    )

    # -----------------------------------------------------------
    # 第一步：重构数据结构（restructure）
    #
    # 将原始 RLDS 字段重新整理成统一格式，例如：
    # {
    #   "observation": {...},
    #   "action": ...,
    #   "task": {...}
    # }
    #
    # 这是轨迹级别操作，因此使用 traj_map
    # -----------------------------------------------------------
    dataset = dataset.traj_map(
        restructure,
        num_parallel_calls
    )

    # -----------------------------------------------------------
    # 第二步：对 action 和 proprio 进行归一化
    #
    # normalize_action_and_proprio:
    #   - 使用预先计算好的 dataset_statistics
    #   - 按指定 normalization_type 进行标准化
    #
    # 常见归一化方式：
    #   - mean/std 标准化
    #   - min/max 归一化
    #
    # 归一化的目的：
    #   - 不同机器人/不同数据源数值尺度不同
    #   - 便于模型稳定训练
    # 🔹 对整条轨迹（trajectory）中的 action 和 proprio 做数值归一化（normalization）
    # 在多机器人、多数据源训练（比如 OpenVLA）中，不同数据的数值范围差异很大：有的机器人 Δx 单位是米
    # 有的是厘米 有的关节角度范围 [-3.14, 3.14] 有的只在 [-0.5, 0.5] 如果不归一化，模型会非常难训练。
    # 同样是轨迹级别操作 
    # -----------------------------------------------------------
    dataset = dataset.traj_map(
        partial(
            normalize_action_and_proprio,
            metadata=dataset_statistics,
            normalization_type=action_proprio_normalization_type,
        ),
        num_parallel_calls,
    )

    # -----------------------------------------------------------
    # 返回：
    #   dataset            → 处理后的数据集
    #   dataset_statistics → 数据统计信息（供后续反归一化或推理使用）
    # -----------------------------------------------------------
    return dataset, dataset_statistics

```
- 先加载或计算每个数据集的统计量（如均值方差、分位数边界等）。
- 再根据归一化类型和 mask 决定对哪些维度做缩放。

真正执行在 `prismatic/vla/datasets/rlds/dataset.py:264`：
- 通过 `normalize_action_and_proprio(...)` 对 `action` 和 `proprio` 做标准化。 
| metadata里的key | traj中的路径                       |
| ------------- | ------------------------------ |
| action        | traj["action"]                 |
| proprio       | traj["observation"]["proprio"] |

- OpenVLA 默认常用 `BOUNDS_Q99`（基于分位数边界）来降低异常值影响。
| 类型         | 公式        | 输出范围   | 适用场景         |
| ---------- | --------- | ------ | ------------ |
| NORMAL     | (x-μ)/σ   | (-∞,∞) | 高斯分布数据       |
| BOUNDS     | Min-Max   | [-1,1] | 控制信号         |
| BOUNDS_Q99 | 分位Min-Max | [-1,1] | 多机器人混合数据（推荐） |

这一步的训练价值很直接：
1. 不同数据源的动作尺度被拉到可比较范围，混合训练更稳定。
2. 梯度不会被少数大幅度动作主导，优化更平滑。
3. 推理时可结合保存的统计量做反归一化，恢复到真实机器人控制量。


## 3.统一样本结构（关键）
关键代码
```python
def make_dataset_from_rlds(
    name: str,
    data_dir: str,
    *,
    train: bool,
    standardize_fn: Optional[Callable[[dict], dict]] = None,
    shuffle: bool = True,
    image_obs_keys: Dict[str, Optional[str]] = {},
    depth_obs_keys: Dict[str, Optional[str]] = {},
    state_obs_keys: List[Optional[str]] = (),
    language_key: Optional[str] = None,
    action_proprio_normalization_type: NormalizationType = NormalizationType.NORMAL,
    dataset_statistics: Optional[Union[dict, str]] = None,
    absolute_action_mask: Optional[List[bool]] = None,
    action_normalization_mask: Optional[List[bool]] = None,
    num_parallel_reads: int = tf.data.AUTOTUNE,
    num_parallel_calls: int = tf.data.AUTOTUNE,
) -> Tuple[dl.DLataset, dict]:
    '''
        Returns:
轨迹数据集，其中每个步骤包含以下字段：
        - observation（观测数据）:
            - image_{name1, name2, ...} # RGB图像观测数据（注：花括号内为不同相机/视角的名称，如camera_left、camera_top等）
            - depth_{name1, name2, ...} # 深度图像观测数据（注：与RGB图像一一对应，记录场景的深度信息）
            - proprio                   # 本体感受观测数据的一维数组（注：如机器人关节角度、速度、力反馈等自身状态数据）
            - timestep                  # 每一帧对应的时间步（注：标识该观测数据在轨迹序列中的时间位置）
        - task（任务信息）:
            - language_instruction      # 语言指令（注：仅当指定了`language_key`参数时才会存在该字段）
        - action                        # 动作向量（注：该时间步执行的动作，如机器人的关节控制指令、移动指令等）
        - dataset_name                  # 数据集名称（注：标识该条轨迹所属的数据集，用于多数据集融合场景） 
    '''
```
make_dataset_from_rlds 会把所有数据源整理成同样字段：
observation（图像/可选depth/proprio）
task.language_instruction
action
dataset_name
见：dataset.py (line 59), dataset.py (line 206)

## 4.统一轨迹/帧级处理（数据清洗）
### 统一轨迹
#### 核心库dlimp

dlimp 是一个专门用于处理 ** 轨迹数据集（Trajectory Datasets）** 的 Python 库，常见于机器人学习、强化学习或具身智能（VLA）相关的研究中。
核心功能：
DLataset 类：对 tf.data.Dataset 的轻量级封装，专为轨迹数据设计，支持从 TFRecords 或 RLDS 格式加载数据。
数据转换工具：提供 frame_map（帧级转换）和 traj_map（轨迹级转换）方法，方便对轨迹数据进行预处理。
数据集转换脚本：包含将其他格式数据集转换为 dlimp 兼容格式的工具（推荐使用 RLDS 格式）。
简单说，它是用来高效加载、处理和转换机器人 / 智能体轨迹数据的工具库。
#### openvla具体实现
```python
def apply_trajectory_transforms(
    dataset: dl.DLataset,
    *,
    train: bool,
    goal_relabeling_strategy: Optional[str] = None,
    goal_relabeling_kwargs: dict = {},
    window_size: int = 1,
    future_action_window_size: int = 0,
    subsample_length: Optional[int] = None,
    skip_unlabeled: bool = False,
    max_action: Optional[float] = None,
    max_proprio: Optional[float] = None,
    task_augment_strategy: Optional[str] = None,
    task_augment_kwargs: dict = {},
    num_parallel_calls: int = tf.data.AUTOTUNE,
) -> dl.DLataset:
    """
    在“轨迹级别”应用通用的数据变换。这类变换通常属于“重标注”类操作
    （例如过滤、分块、添加目标、删除字段等）。

    本函数中的变换应满足以下特性：
        - 需要访问完整轨迹（不能逐帧独立处理）。
        - 通常不是 CPU 密集型操作，主要是数据移动或复制。
        - 不需要解码图像。

    参数说明：
        dataset (dl.DLataset): 待处理的数据集。
        train (bool): 是否为训练集（影响是否进行子采样等操作）。
        goal_relabeling_strategy (str, 可选): 目标重标注策略名称；若为 None，则不进行目标重标注。详见 `goal_relabeling.py`。"her"（Hindsight Experience Replay）、"future"（未来帧）、"final"（最后一帧）
        goal_relabeling_kwargs (dict, 可选): 重标记策略（对应 goal_relabeling 模块里的函数名）传给重标记函数的参数字典	
        window_size (int, 可选): 将轨迹切分成片段时，每个片段的长度。10（输入过去 10 帧的状态）
        future_action_window_size (int, 可选): 未来动作窗口：把当前帧之后 N 步的动作也拼接到样本中（常用于多步预测 / 扩散模型）5（拼接未来 5 步的动作）
        subsample_length (int, 可选): 若指定，则在目标重标注与分块之后，将长度超过该值的轨迹子采样为该长度。将长轨迹截断 / 下采样到固定长度（避免轨迹过长爆显存）
        skip_unlabeled (bool, 可选): 是否跳过没有语言标注的轨迹。
        max_action (float, 可选): 若指定，则当任一轨迹中任一时间步的任一动作维度
            的绝对值超过该阈值时，丢弃该轨迹。
        max_proprio (float, 可选): 若指定，则当任一轨迹中任一时间步的任一
            本体感觉（proprio）维度的绝对值超过该阈值时，丢弃该轨迹。
        task_augment_strategy (str, 可选): 任务增强策略名称；
            若为 None，则不进行任务增强。详见 `task_augmentation.py`。增强策略（动态加载 task_augmentation 模块里的函数）
        task_augment_kwargs (dict, 可选): 传递给任务增强函数的额外参数。
        num_parallel_calls (int, 可选): map 操作的并行调用数，默认 AUTOTUNE。并行处理的线程数	tf.data.AUTOTUNE（自动调优）
    """

    # 若设置跳过无语言标注数据
    if skip_unlabeled:
        # 检查数据集中是否包含语言字段
        if "language_instruction" not in dataset.element_spec["task"]:
            raise ValueError("skip_unlabeled=True 但数据集中不存在语言标注字段。")

        # 仅保留 language_instruction 非空的轨迹
        dataset = dataset.filter(
            lambda x: tf.math.reduce_any(x["task"]["language_instruction"] != "")
        )

    # 若设置最大动作阈值，则过滤超出范围的轨迹
    if max_action is not None:
        dataset = dataset.filter(
            lambda x: tf.math.reduce_all(
                tf.math.abs(x["action"]) <= max_action
            )
        )

    # 若设置最大 proprio 阈值，则过滤超出范围的轨迹
    if max_proprio is not None and "proprio" in dataset.element_spec["observation"]:
        dataset = dataset.filter(
            lambda x: tf.math.reduce_all(
                tf.math.abs(x["observation"]["proprio"]) <= max_proprio
            )
        )

    # 标记 observation 和 task 字典中哪些条目是 padding
    dataset = dataset.traj_map(
        traj_transforms.add_pad_mask_dict,
        num_parallel_calls
    )

    # 更新 "task" 字典（进行目标重标注）
    if goal_relabeling_strategy is not None:
        # traj_map 是 dlimp 中针对整段轨迹的批量处理方法，需传入接收 / 返回轨迹字典的自定义函数
        dataset = dataset.traj_map(
            partial(
                getattr(goal_relabeling, goal_relabeling_strategy),
                **goal_relabeling_kwargs
            ),
            num_parallel_calls,
        )

    # 任务增强必须在分块之前执行（防止修改 goal timestep 后出错）
    if train and task_augment_strategy is not None:
        # 执行任务增强（例如删除某些键）
        dataset = dataset.traj_map(
            partial(
                getattr(task_augmentation, task_augment_strategy),
                **task_augment_kwargs,
            ),
            num_parallel_calls,
        )

    # 对 observation 和 action 进行分块：
    # - observation 在第 1 维新增一个长度为 window_size 的维度
    # - action 在第 1 维新增一个长度为 window_size + future_action_window_size 的维度
    dataset = dataset.traj_map(
        partial(
            traj_transforms.chunk_act_obs,
            window_size=window_size,
            future_action_window_size=future_action_window_size,
        ),
        num_parallel_calls,
    )

    # 若为训练模式并指定子采样长度，则进行轨迹子采样
    if train and subsample_length is not None:
        dataset = dataset.traj_map(
            partial(
                traj_transforms.subsample,
                subsample_length=subsample_length
            ),
            num_parallel_calls,
        )

    return dataset
```
4.1. **功能定位**：`apply_trajectory_transforms` 是针对轨迹级别的数据集转换函数，主要完成过滤、重标记、分块、子采样等操作，仅处理完整轨迹数据，不涉及图像解码，计算开销低。

4.2. **核心操作流程**：
   - 先过滤：剔除无语言标签（`skip_unlabeled`）、动作/本体感知数据超限（`max_action`/`max_proprio`）的轨迹；
   - 再标记：为观测/任务字典添加填充掩码；
   - 后重标记/增强：按策略更新目标标签（`goal_relabeling`），训练模式下执行任务增强（`task_augmentation`）；
   - 最后分块/子采样：将轨迹切分为指定窗口大小的片段，训练模式下对超长轨迹进行子采样至指定长度。

4.3. **关键参数控制**：
   - 训练/测试模式（`train`）影响任务增强和子采样是否执行；
   - 并行调用数（`num_parallel_calls`）控制所有映射操作的并行度，默认自动适配；
   - 窗口参数（`window_size`/`future_action_window_size`）决定轨迹分块的长度维度。

4.4
- 函数仅处理**完整轨迹**，操作以过滤、重标记、分块为主，不涉及逐帧处理和图像解码；
- 执行顺序有严格要求：任务增强需在分块前、目标重标记后完成；
- 核心过滤逻辑针对无标签数据、超限动作/本体感知数据，核心转换逻辑针对目标标签、任务增强、轨迹分块和子采样。

### 帧级别处理
与处理轨迹类似.**注意应该先处理帧数据后进行轨迹数据处理**
```python
def apply_frame_transforms(
    dataset: dl.DLataset,
    *,
    train: bool,
    image_augment_kwargs: Union[Dict, Dict[str, Dict]] = {},
    resize_size: Union[Tuple[int, int], Dict[str, Tuple[int, int]]] = {},
    depth_resize_size: Union[Tuple[int, int], Dict[str, Tuple[int, int]]] = {},
    num_parallel_calls: int = tf.data.AUTOTUNE,
) -> dl.DLataset:
    """
    在“帧级别（frame level）”对数据进行通用变换。
    这类变换通常计算量更大，例如图像解码、图像缩放、数据增强等。

    参数说明：
        train (bool): 是否为训练模式（影响是否执行图像增强）。
        dataset (dl.DLataset): 待处理的数据集。
        image_augment_kwargs (dict | Mapping[str, dict]):
            传递给图像增强函数的参数。详见 `dlimp.transforms.augment_image`。
            - 如果是一个普通 dict，则应用于所有图像。
            - 如果是 dict[str, dict]，例如 {"wrist": {...}}，
              则会作用于 "image_wrist"（名称由 `make_dataset_from_rlds` 中的 image_obs_keys 决定）。
            - 若某些 key 不存在，则跳过对应增强。
            - 若传入空 dict，则跳过所有图像增强。
        resize_size (Tuple[int, int] | Mapping[str, Tuple[int, int]]):
            图像 resize 的目标尺寸。
            - 若为单个 (H, W)，则应用于所有图像。
            - 若为 dict[str, (H, W)]，则针对 "image_{k}" 分别 resize。
            - 若为空 dict，则跳过所有 resize。
        depth_resize_size (Tuple[int, int] | Mapping[str, Tuple[int, int]]):
            与 resize_size 相同，但用于深度图像。
        num_parallel_calls (int):
            frame_map 操作的并行调用数量，默认 AUTOTUNE。
    """

    # -----------------------------------------------------------
    # 工具函数：将一个作用在“非分块 observation 字典”上的函数 fn，
    # 同时应用到：
    #   1. 非分块的 task 字典
    #   2. 分块（chunked）的 observation 字典
    #
    # dl.vmap(fn) 表示对 observation 中的每个时间步（或窗口）应用 fn
    # -----------------------------------------------------------
    def apply_obs_transform(fn: Callable[[Dict], Dict], frame: Dict) -> Dict:
        # 对 task 字典直接应用变换
        frame["task"] = fn(frame["task"])

        # 对 observation 字典中的每个时间步分别应用变换
        frame["observation"] = dl.vmap(fn)(frame["observation"])
        return frame

    # -----------------------------------------------------------
    # 第一步：图像解码 + resize（包括 RGB 和 depth）
    # 这是 CPU 开销较大的步骤
    # -----------------------------------------------------------
    dataset = dataset.frame_map(
        partial(
            apply_obs_transform,
            partial(
                obs_transforms.decode_and_resize,
                resize_size=resize_size,
                depth_resize_size=depth_resize_size,
            ),
        ),
        num_parallel_calls,
    )

    # -----------------------------------------------------------
    # 第二步：若为训练模式，进行图像数据增强
    # 所有图像使用同一个随机种子（保证多视角一致增强）
    # 并跳过 padding 图像
    # -----------------------------------------------------------
    if train:

        def aug(frame: dict):
            # 为当前 frame 生成一个随机种子（长度为2的 int32 向量）
            seed = tf.random.uniform(
                [2],
                maxval=tf.dtypes.int32.max,
                dtype=tf.int32
            )

            # 构造增强函数
            aug_fn = partial(
                obs_transforms.augment,
                seed=seed,
                augment_kwargs=image_augment_kwargs
            )

            # 将增强函数应用到 task 和 observation
            return apply_obs_transform(aug_fn, frame)

        dataset = dataset.frame_map(aug, num_parallel_calls)

    return dataset
```

## 5.多数据源混合采样
### 多数据源+权重

- 在 OXE_NAMED_MIXTURES（数据集+权重）给不同的数据集设置不同的权重
```python
OXE_NAMED_MIXTURES: Dict[str, List[Tuple[str, float]]] = {
    # === Bridge V2 Dataset ===
    "bridge": [
        # ("bridge_oxe", 1.0),                                    # Version of Bridge V2 in Open-X GCP Bucket
        ("bridge_orig", 1.0),                                   # Original Version of Bridge V2 from Project Website
    ],

```
### 按权重交错采样换个balance
```python
# === Core Initializer ===
def make_interleaved_dataset(
    dataset_kwargs_list: List[Dict],
    sample_weights: Optional[List[float]] = None,
    *,
    train: bool,
    shuffle_buffer_size: int,
    traj_transform_kwargs: Optional[Dict] = None,
    frame_transform_kwargs: Optional[Dict] = None,
    batch_size: Optional[int] = None,
    balance_weights: bool = False,
    traj_transform_threads: Optional[int] = None,
    traj_read_threads: Optional[int] = None,
) -> dl.DLataset:
    ...
    # Default to uniform sampling (if `sample_weights` is not specified)
    if not sample_weights:
        sample_weights = [1.0] * len(dataset_kwargs_list)
    ...
    # Balance and Normalize Weights
    if balance_weights:
        sample_weights = np.array(sample_weights) * np.array(dataset_sizes)
    sample_weights = np.array(sample_weights) / np.sum(sample_weights)
    ...
```
- 这样混合采样训练的好处：
    - 防止小数据集被大数据集淹没。
        纯拼接会让大数据集主导梯度；加权交错能保证小而关键的数据源持续被看到。
    - 控制“先验偏好”与“数据规模”。
        你可以先用 mixture 给任务优先级，再用 balance_weights 考虑规模，得到更合理的采样分布。
    - 训练更稳、泛化更好。
        多分布持续交替喂给模型，能减少对单一分布过拟合，跨场景/跨机器人表现通常更稳。
    - 训练吞吐更可控。
## 6.构建模型数据加载器
核心函数-make_interleaved_dataset 讲之前的1-5从数据源注册、权重设置、动作统一、统一轨迹/帧级处理、动作分块等按流程顺序梳理。
# 模型构建
openvla在实验过程中进行了大量的实验，为我们提供了宝贵的经验。
在模型架构层面：

- 视觉模型的技术选型：只使用SigLiP的视觉编码器和采用 SigLiP+DinoV2视觉编码器
- Projector映射层连接视觉层和语言层
在模型训练层面：
- 微调冻结不同层：对全量，视觉模型参数，最后一层、projector层等冻结的性能测试
从以上需求出发，openvla不仅有良好的数据解耦兼容性，还有着良好的模型解耦兼容性，支持配置式选择不同训练阶段，支持配置式更新不同层的参数。整个框架都值得我们在AI训练业务中去学习。
在模型实验层面：
- 量化模型对性能的影响：在8bit和16bit对性能的影响

## 1.模型构建总览（Prismatic）

## 2.模块接口与数据流

## 3.视觉编码器设计（SigLIP vs SigLIP + DINOv2）

## 4.LLM主干与多模态注入位置

## 5.Projector设计与作用

## 6.Action预测头与损失设计

## 7.训练配置与冻结策略（全量/视觉/末层/Projector）

## 8.量化训练与精度权衡（8bit vs 16bit）

## 9.消融结论与工程落地建议

# 仿真

# 训练和微调流程

# 性能测试
